{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# class DifferentiableCropLayer(tf.keras.layers.Layer):\n",
    "#     def __init__(self, target_size=(15, 15)):\n",
    "#         super(DifferentiableCropLayer, self).__init__()\n",
    "#         self.target_size = target_size\n",
    "    \n",
    "#     def call(self, images):\n",
    "#         # Assuming the input images are grayscale and normalized [0, 1]\n",
    "#         def crop_and_resize(image):\n",
    "#             # Create a mask where the pixels are non-zero\n",
    "#             mask = tf.cast(image > 0.1, tf.float32)\n",
    "            \n",
    "#             # Calculate the bounding box\n",
    "#             x_proj = tf.reduce_sum(mask, axis=0)\n",
    "#             y_proj = tf.reduce_sum(mask, axis=1)\n",
    "            \n",
    "#             x_nonzero = tf.where(x_proj > 0)\n",
    "#             y_nonzero = tf.where(y_proj > 0)\n",
    "            \n",
    "#             x_min = tf.reduce_min(x_nonzero)\n",
    "#             x_max = tf.reduce_max(x_nonzero)\n",
    "#             y_min = tf.reduce_min(y_nonzero)\n",
    "#             y_max = tf.reduce_max(y_nonzero)\n",
    "            \n",
    "#             # Crop the image using the bounding box, considering tensor shape\n",
    "#             cropped_image = tf.image.crop_to_bounding_box(image,\n",
    "#                                                           offset_height=y_min,\n",
    "#                                                           offset_width=x_min,\n",
    "#                                                           target_height=y_max - y_min + 1,\n",
    "#                                                           target_width=x_max - x_min + 1)\n",
    "#             # Resize to target size\n",
    "#             resized_image = tf.image.resize(cropped_image, self.target_size, method='bilinear')\n",
    "#             return resized_image\n",
    "        \n",
    "#         # Apply the crop_and_resize to each image in the batch\n",
    "#         cropped_resized_images = tf.map_fn(crop_and_resize, images, dtype=tf.float32)\n",
    "#         return cropped_resized_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:53:09.327259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-01 17:53:09.340831: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-01 17:53:09.347252: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-01 17:53:09.368929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-01 17:53:10.299678: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # This disables GPU usage in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 17:53:13.162915: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-08-01 17:53:13.162980: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:135] retrieving CUDA diagnostic information for host: shenron.urcf.drexel.edu\n",
      "2024-08-01 17:53:13.162988: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:142] hostname: shenron.urcf.drexel.edu\n",
      "2024-08-01 17:53:13.163122: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:166] libcuda reported version is: 550.90.7\n",
      "2024-08-01 17:53:13.163170: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:170] kernel reported version is: 550.90.7\n",
      "2024-08-01 17:53:13.163175: I external/local_xla/xla/stream_executor/cuda/cuda_diagnostics.cc:249] kernel version seems to match DSO: 550.90.7\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)  # Memory growth must be set before GPUs have been initialized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "    # Normalizing and resizing MNIST digits to be placed in a 256x256 image\n",
    "    def place_digit(image):\n",
    "        size = 256\n",
    "        placement = np.zeros((size, size), dtype=np.float32)\n",
    "        x_offset = np.random.randint(0, size - 28)\n",
    "        y_offset = np.random.randint(0, size - 28)\n",
    "        placement[y_offset:y_offset+28, x_offset:x_offset+28] = image\n",
    "        return placement, [x_offset / size, y_offset / size, (x_offset + 28) / size, (y_offset + 28) / size]\n",
    "\n",
    "    # Applying the function and separating results\n",
    "    train_results = [place_digit(img) for img in train_images]\n",
    "    test_results = [place_digit(img) for img in test_images]\n",
    "\n",
    "    # Normalize and reshape images\n",
    "    train_images = np.array([res[0] for res in train_results]) / 255.0\n",
    "    train_bboxes = np.array([res[1] for res in train_results])\n",
    "    test_images = np.array([res[0] for res in test_results]) / 255.0\n",
    "    test_bboxes = np.array([res[1] for res in test_results])\n",
    "\n",
    "    # Expand dimensions to include channel information\n",
    "    train_images = np.expand_dims(train_images, axis=-1)\n",
    "    test_images = np.expand_dims(test_images, axis=-1)\n",
    "\n",
    "    return (train_images, train_bboxes, train_labels), (test_images, test_bboxes, test_labels)\n",
    "\n",
    "(train_images, train_bboxes, train_labels), (test_images, test_bboxes, test_labels) = create_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1ccd606910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_size=(28, 28), padding_mode='CONSTANT'):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.padding_mode = padding_mode\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # This function can help manage dimensions better and is a good place to initialize weights if needed\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        images, coords = inputs\n",
    "        shape = tf.cast(tf.shape(images), tf.float32)\n",
    "        center_x, center_y, width, height = tf.split(coords, 4, axis=1)\n",
    "        half_width = width * shape[2] / 2\n",
    "        half_height = height * shape[1] / 2\n",
    "\n",
    "        x_min = tf.cast(center_x * shape[2] - half_width, tf.int32)\n",
    "        y_min = tf.cast(center_y * shape[1] - half_height, tf.int32)\n",
    "        x_max = tf.cast(center_x * shape[2] + half_width, tf.int32)\n",
    "        y_max = tf.cast(center_y * shape[1] + half_height, tf.int32)\n",
    "\n",
    "        crop_height = tf.maximum(y_max - y_min, 1)\n",
    "        crop_width = tf.maximum(x_max - x_min, 1)\n",
    "\n",
    "        cropped_images = tf.image.crop_to_bounding_box(images, y_min[0], x_min[0], crop_height[0], crop_width[0])\n",
    "        pad_height = tf.maximum(self.target_size[0] - crop_height, 0)\n",
    "        pad_width = tf.maximum(self.target_size[1] - crop_width, 0)\n",
    "        \n",
    "        pad_top = pad_height // 2\n",
    "        pad_bottom = pad_height - pad_top\n",
    "        pad_left = pad_width // 2\n",
    "        pad_right = pad_width - pad_left\n",
    "\n",
    "        padded_images = tf.pad(cropped_images, [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]], mode=self.padding_mode)\n",
    "        return padded_images\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Ensure the output shape is correctly defined based on the target size, assuming the last dimension remains unchanged\n",
    "        return (input_shape[0][0], *self.target_size, input_shape[0][-1])\n",
    "\n",
    "\n",
    "\n",
    "# Usage Example\n",
    "# Assuming 'model_input' and 'bbox_coords' are your input images and bounding box coordinates\n",
    "# crop_layer = CropLayer(target_size=(28, 28))\n",
    "# cropped_and_padded_images = crop_layer([model_input, bbox_coords])\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    input_layer = Input(shape=(256, 256, 1))\n",
    "    \n",
    "    # Base convolutional layers\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_layer)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Localization branch predicts coordinates for cropping\n",
    "    localization = Dense(128, activation='relu')(x)\n",
    "    coords = Dense(4, activation='sigmoid', name='localization_output')(localization)  # Named output for localization\n",
    "    \n",
    "    # Cropping layer\n",
    "    crop_layer = CropLayer()(inputs=[input_layer, coords])\n",
    "    \n",
    "    # Classification branch\n",
    "    classification_input = Flatten()(crop_layer)\n",
    "    classification = Dense(128, activation='relu')(classification_input)\n",
    "    classification = Dropout(0.5)(classification)\n",
    "    classification_output = Dense(10, activation='softmax', name='classification_output')(classification)  # Named output for classification\n",
    "    \n",
    "    # Construct the full model\n",
    "    model = Model(inputs=input_layer, outputs=[coords, classification_output])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'localization_output': 'mean_squared_error',  # Corrected to the actual output layer name\n",
    "        'classification_output': 'sparse_categorical_crossentropy'  # Corrected to the actual output layer name\n",
    "    },\n",
    "    loss_weights={\n",
    "        'localization_output': 1.0,  # Ensure the weight matches the importance of the task\n",
    "        'classification_output': 1.0  # Ensure the weight matches the importance of the task\n",
    "    },\n",
    "    metrics={\n",
    "        'localization_output': ['mae'],  # Metrics for the localization output\n",
    "        'classification_output': ['accuracy']  # Metrics for the classification output\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling CropLayer.call().\n\n\u001b[1mShapes must be equal rank, but are 1 and 0\n\tFrom merging shape 2 with other shapes. for '{{node functional_6_1/crop_layer_13_1/crop_to_bounding_box/stack_2}} = Pack[N=4, T=DT_INT32, axis=0](functional_6_1/crop_layer_13_1/crop_to_bounding_box/stack_2/values_0, functional_6_1/crop_layer_13_1/strided_slice_6, functional_6_1/crop_layer_13_1/strided_slice_7, functional_6_1/crop_layer_13_1/crop_to_bounding_box/stack_2/values_3)' with input shapes: [], [1], [1], [].\u001b[0m\n\nArguments received by CropLayer.call():\n  • inputs=['tf.Tensor(shape=(32, 256, 256, 1), dtype=float32)', 'tf.Tensor(shape=(32, 4), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalization_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalization_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_bboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclassification_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[36], line 26\u001b[0m, in \u001b[0;36mCropLayer.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     23\u001b[0m crop_height \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(y_max \u001b[38;5;241m-\u001b[39m y_min, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m crop_width \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(x_max \u001b[38;5;241m-\u001b[39m x_min, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m cropped_images \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_to_bounding_box\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_min\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_min\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_height\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_width\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m pad_height \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m crop_height, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m pad_width \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m crop_width, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling CropLayer.call().\n\n\u001b[1mShapes must be equal rank, but are 1 and 0\n\tFrom merging shape 2 with other shapes. for '{{node functional_6_1/crop_layer_13_1/crop_to_bounding_box/stack_2}} = Pack[N=4, T=DT_INT32, axis=0](functional_6_1/crop_layer_13_1/crop_to_bounding_box/stack_2/values_0, functional_6_1/crop_layer_13_1/strided_slice_6, functional_6_1/crop_layer_13_1/strided_slice_7, functional_6_1/crop_layer_13_1/crop_to_bounding_box/stack_2/values_3)' with input shapes: [], [1], [1], [].\u001b[0m\n\nArguments received by CropLayer.call():\n  • inputs=['tf.Tensor(shape=(32, 256, 256, 1), dtype=float32)', 'tf.Tensor(shape=(32, 4), dtype=float32)']"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_images, \n",
    "    {\n",
    "        'localization_output': train_bboxes, \n",
    "        'classification_output': train_labels\n",
    "    },\n",
    "    validation_data=(\n",
    "        test_images, \n",
    "        {\n",
    "            'localization_output': test_bboxes, \n",
    "            'classification_output': test_labels\n",
    "        }\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
